---
layout: post
title: 音频总线专题（一）——基本知识
date: 2017-08-28
categories: AUDIO
tags: [AUDIO,APPLICATION,BUS]
description: AUDIO STUDY AND APPLICATION
---

#### **1.采样和采样频率：**

在音频处理时要先把音频的模拟信号变成数字信号，即A/D转换。要把音频的模拟信号变成数字信号，就需要采样（抽样）。把音频播放出来时则需要把数字信号转换成模拟信号，即D/A转换。

**采样频率：一秒钟内采样的次数**。

根据奈奎斯特采样定理，要想重建原始信号，采样频率必须大于信号中最高频率的两倍。采样频率越高，采样点之间的间隔就越小，越接近原始信号，数字化后得到的声音就越逼真，但是也加大了运算处理的复杂度。人能感受到的频率范围为 20HZ--20kHZ, 一般音乐的采样频率为44.1kHZ(根据奈奎斯特采样定理,采样频率大于信号中最高频率的两倍), 更高的可以是48kHZ和96kHZ，不过一般人用耳听感觉不出差别了。语音主要是以沟通为主，不需要像音乐那样清晰，分窄带和宽带。**窄带频率范围为300Hz--3400Hz,相应的采样频率为8000Hz;** 宽带频率范围为50Hz--7000Hz,相应的采样频率为16000Hz，用16k采样的语音就称为高清语音了。**现在主流的语音采样频率为16kHz。**

#### **2.采样位数：**

数字信号是用0和1来表示的。**采样位数就是采样值用多少位0和1来表示**，也叫采样精度，用的位数越多就越接近真实声音。如用8位表示，采样值取值范围就是-128--127，如用16位表示，采样值取值范围就是-32768--32767。现在一般都用16位采样位数。

#### **3.声道（channel）：**

通常语音只用一个声道。而对于音乐来说，既可以是单声道（mono），也可以是双声道（即左声道右声道，叫立体声stereo），还可以是多声道，叫环绕立体声，多用于影院中。

不经过压，声音数据量的计算公式为：
**数据量（字节/秒）= (采样频率（Hz）× 采样位数（bit） × 声道数)/ 8**

#### **4.音频采集和播放**：

一般用专门的芯片（codec芯片）采集音频，做A/D转换，然后把数字信号通过I2S总线（主流用I2S总线，也有如PCM总线）送给CPU处理（也有的会把codec芯片与CPU芯片集成在一块芯片中）。当要播放时CPU会把音频数字信号通过I2S总线送给codec芯片，然后做D/A转换得到模拟信号再播放出来。这部分对语音和音乐是通用的，只是用的采样率有可能不一样，音乐的采样率用的高一些。

#### **5.编解码：**

如果把采样值直接保存或者发送，会占用很大的存储空间或者很大的流量。以16kHz采样率16位采样位数单声道为例，一秒钟就有32000（2字节*16000）字节。通常需要把采样后的数字信号压缩后才保存或者发送。把采样值压缩——编码（encode），形成比特流（bitstream）。 把比特流还原出采样值——解码（decode），统称编解码（codec）。

音频解码器分硬解码器和软解码器二种。软解码器是以软件的方式实现解码，如计算机中安装的音频播放软件中就带着软解码器。而硬解码器则是将解码的工作交给特定的解码芯片来完成。

#### **6.音频的编解码**

通常也把音频采样过程叫脉冲编码调制编码，即PCM（Pulse Code Modulation）编码，采样值称PCM值。为了节省保存空间或者发送流量，会对PCM值压缩。目前主要有三大技术标准组织制定压缩标准：

（1）ITU，主要制定有线语音的压缩标准（g系列），有g711/g722/g726/g729等。
（2）3GPP,主要制定无线语音的压缩标准（amr系列等）, 有amr-nb/amr-wb。后来ITU吸纳了amr-wb，形成了g722.2。
（3）MPEG,主要制定音乐的压缩标准，有11172-3，13818-3/7，14496-3等。



#### **7.无损压缩和有损压缩：**

把PCM数据压缩后无任何损伤叫无损压缩，不过压缩程度不高。把PCM数据压缩后有损伤叫有损压缩，最多可以压到几十分之一，不过音频质量差些。

#### **8.音频前后处理：**

音频处理是指对PCM数据（也叫线性数据）进行处理，从而达到想要的效果，如回声消除。对音频编码前的PCM数据进行处理叫音频前处理，主要用于语音中，来去除各种干扰，使声音更清晰，主要有回声消除、噪声抑制、增益控制等。
对音频解码后的PCM数据进行处理叫音频后处理，主要用于音乐中，来产生各种音效，使音乐更动听，主要有均衡器、混响等。

#### **9.音频传输**

这里主要是指网络传输，通过网络把音频数据传给对方。语音和音乐两种场景下有明显的区别。对于语音来说，实时性要求很高，主要用RTP/UDP做承载，由于UDP是不可靠传输，会丢包乱序等，影响语音质量，所以要采取相应的措施，主要有PLC(丢包补偿)、FEC(前向纠错)、重传、jitter buffer等。

对于音乐来说，以前是播放本地音乐文件，近些年随着网络带宽的加大，可以播放云端的音乐文件了。播放时要把音乐文件传给播放器，一般是边播放边下载，播放音乐对实时性要求不高，一般用HTTP/TCP做承载，也就不存在丢包乱序等问题了。

## 参考：

1.[音频开发基础知识简介 ](http://www.cnblogs.com/talkaudiodev/p/7041477.html)

2.[音频采样位数，采样率，比特率](http://blog.csdn.net/iloli/article/details/19252103)

